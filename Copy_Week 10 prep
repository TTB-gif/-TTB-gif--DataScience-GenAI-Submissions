{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+jbyCDAEkhTHOgOo92SPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TTB-gif/-TTB-gif--DataScience-GenAI-Submissions/blob/main/Copy_Week%2010%20prep\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knuj1x9S1KOT"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('loan_data.csv')\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "KJIpjri51TqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('DataFrame Info:')\n",
        "df.info()\n",
        "\n",
        "print('\\nMissing Values (Count):')\n",
        "display(df.isnull().sum()) # CHECKING FOR EMPTY CELLS"
      ],
      "metadata": {
        "id": "YNfK8bMt1UO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)\n",
        "df.isnull().sum() # REMOVE ALL EMPTY CELLS"
      ],
      "metadata": {
        "id": "zsf6sglm35Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"Id\"], axis=1) # REMOVE NON-PREDICTIVE COLLUMS\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Spc5CNUR38jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data prep above**\n",
        "\n"
      ],
      "metadata": {
        "id": "P6J4HIDAdUUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean data\n",
        "# Split data into training? - need to remove target collum atleast - sep into x,y vals\n",
        "# train models\n",
        "#print matrices\n",
        "#print bar charts"
      ],
      "metadata": {
        "id": "dxmsL0Tu4EwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'loan_status' is the target variable (y) and the rest are features (X)\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "# Split the data into 70% training and 30% testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "nhg4sZbKdMaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg_model = LogisticRegression(random_state=42, solver='liblinear') # 'liblinear' is a good choice for smaller datasets or L1/L2 regularization\n",
        "\n",
        "# Train the model\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained successfully!\")"
      ],
      "metadata": {
        "id": "MRmJb_4idSg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "y_pred_proba = log_reg_model.predict_proba(X_test)[:, 1] # Probability of the positive class (loan_status = 1)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "wLmLcdZqdTbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot a confusion matrix for the model\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=log_reg_model.classes_)\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=log_reg_model.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for Logistic Regression Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CJak0d3Hdfit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\"Logistis Regression above"
      ],
      "metadata": {
        "id": "nH5reuoOeNU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the RandomForestClassifier with specified hyperparameters\n",
        "# n_estimators: The number of trees in the forest. A higher number generally improves performance but increases computation time.\n",
        "# random_state: Controls the randomness of the bootstrapping of the samples used when building trees and the sampling of the features to consider when looking for the best split at each node. Essential for reproducibility.\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier using the training data\n",
        "rf_classifier.fit(X_train, Y_train)\n",
        "print(\"RandomForestClassifier trained successfully.\")"
      ],
      "metadata": {
        "id": "vTXf1p_JeLDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_rf = rf_classifier.predict(X_test)\n"
      ],
      "metadata": {
        "id": "B_NEIKtkp7s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_rf = accuracy_score(Y_test, Y_pred_rf)\n",
        "precision_rf = precision_score(Y_test, Y_pred_rf)\n",
        "recall_rf = recall_score(Y_test, Y_pred_rf)\n",
        "f1_rf = f1_score(Y_test, Y_pred_rf)\n",
        "\n",
        "print(\"RandomForestClassifier Performance:\")\n",
        "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
        "print(f\"Precision: {precision_rf:.4f}\")\n",
        "print(f\"Recall: {recall_rf:.4f}\")\n",
        "print(f\"F1-score: {f1_rf:.4f}\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"\\nClassification Report for RandomForestClassifier:\")\n",
        "print(classification_report(Y_test, Y_pred_rf))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(Y_test, Y_pred_rf), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix for RandomForestClassifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FSn108bSp8NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# random Forrest"
      ],
      "metadata": {
        "id": "Xr0K0uWCp_oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Initialize the GradientBoostingClassifier with specified hyperparameters\n",
        "# n_estimators: The number of boosting stages to perform.\n",
        "# learning_rate: Shrinks the contribution of each tree by `learning_rate`.\n",
        "# random_state: Controls the random seed for reproducibility.\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the classifier using the training data\n",
        "gb_classifier.fit(X_train, Y_train)\n",
        "print(\"GradientBoostingClassifier trained successfully.\")"
      ],
      "metadata": {
        "id": "tNV0cgLVp-cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_gb = gb_classifier.predict(X_test)\n",
        "print(\"Predictions made for GradientBoostingClassifier.\")"
      ],
      "metadata": {
        "id": "PQlicpgLqML6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_gb = accuracy_score(Y_test, Y_pred_gb)\n",
        "precision_gb = precision_score(Y_test, Y_pred_gb)\n",
        "recall_gb = recall_score(Y_test, Y_pred_gb)\n",
        "f1_gb = f1_score(Y_test, Y_pred_gb)\n",
        "\n",
        "print(\"GradientBoostingClassifier Performance:\")\n",
        "print(f\"Accuracy: {accuracy_gb:.4f}\")\n",
        "print(f\"Precision: {precision_gb:.4f}\")\n",
        "print(f\"Recall: {recall_gb:.4f}\")\n",
        "print(f\"F1-score: {f1_gb:.4f}\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"\\nClassification Report for GradientBoostingClassifier:\")\n",
        "print(classification_report(Y_test, Y_pred_gb))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(Y_test, Y_pred_gb), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix for GradientBoostingClassifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qmeG7bDbqOIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grad boosted"
      ],
      "metadata": {
        "id": "772-inULqTCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for GradientBoostingClassifier\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=5 means 5-fold cross-validation will be used.\n",
        "grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation F1-score:\", grid_search.best_score_)\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Define the parameter distributions for GradientBoostingClassifier\n",
        "param_distributions = {\n",
        "    'n_estimators': randint(50, 200),  # Number of boosting stages\n",
        "    'learning_rate': uniform(0.01, 0.2), # Step size shrinkage to prevent overfitting\n",
        "    'max_depth': randint(3, 7)       # Maximum depth of the individual regression estimators\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "# cv=5 means 5-fold cross-validation will be used.\n",
        "# n_iter=50 specifies that 50 different parameter settings will be sampled.\n",
        "# scoring='f1' is used to optimize for the F1-score.\n",
        "# n_jobs=-1 uses all available processors for parallel computation.\n",
        "random_search = RandomizedSearchCV(estimator=gb_classifier, param_distributions=param_distributions,\n",
        "                                   n_iter=50, cv=5, scoring='f1', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, Y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found by RandomizedSearchCV:\", random_search.best_params_)\n",
        "print(\"Best cross-validation F1-score:\", random_search.best_score_)"
      ],
      "metadata": {
        "id": "F9CF3mWjqRvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gridsearch and randomsearch for hyperparameter tuning"
      ],
      "metadata": {
        "id": "jcavMj_wqd8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert DataFrames/Series to TensorFlow tensors\n",
        "X_train_tensor = tf.constant(X_train.values, dtype=tf.float32)\n",
        "X_test_tensor = tf.constant(X_test.values, dtype=tf.float32)\n",
        "y_train_tensor = tf.constant(y_train.values, dtype=tf.float32)\n",
        "y_test_tensor = tf.constant(y_test.values, dtype=tf.float32)\n",
        "\n",
        "# Create TensorFlow Dataset objects\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train_tensor))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_tensor, y_test_tensor))\n",
        "\n",
        "# Batch the datasets\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "print(\"Data successfully converted to TensorFlow tensors and batched datasets created.\")\n",
        "print(f\"Train dataset batch format: {next(iter(train_dataset))}\")\n",
        "print(f\"Test dataset batch format: {next(iter(test_dataset))}\")"
      ],
      "metadata": {
        "id": "BAoWp5owvBtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Obtain one batch from each dataset\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataset))\n",
        "test_features_batch, test_labels_batch = next(iter(test_dataset))\n",
        "\n",
        "print(\"\\n--- Training Dataset Batch Information ---\")\n",
        "print(f\"Shape of training features batch: {train_features_batch.shape}\")\n",
        "print(f\"Data type of training features batch: {train_features_batch.dtype}\")\n",
        "print(f\"Shape of training labels batch: {train_labels_batch.shape}\")\n",
        "print(f\"Data type of training labels batch: {train_labels_batch.dtype}\")\n",
        "\n",
        "print(\"\\n--- Test Dataset Batch Information ---\")\n",
        "print(f\"Shape of test features batch: {test_features_batch.shape}\")\n",
        "print(f\"Data type of test features batch: {test_features_batch.dtype}\")\n",
        "print(f\"Shape of test labels batch: {test_labels_batch.shape}\")\n",
        "print(f\"Data type of test labels batch: {test_labels_batch.dtype}\")"
      ],
      "metadata": {
        "id": "YThttNj2vFYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Initialize the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the input layer explicitly using Input\n",
        "model.add(Input(shape=(15,))) # Define input shape directly\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Add one or more hidden layers\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Add the output layer for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"Neural Network model architecture defined successfully.\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "StSjWVhHvGgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled successfully with Adam optimizer, binary_crossentropy loss, and accuracy metric.\")"
      ],
      "metadata": {
        "id": "Ce3cXDMavMOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    epochs=10, # Using 10 epochs as a starting point\n",
        "                    validation_data=test_dataset)\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "id": "wT4KbiM2vOyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "apQjwe6zvPHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network using tensors"
      ],
      "metadata": {
        "id": "Gl5cAyv6vYh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **If have time look at 6.2 for more neural network stuff**"
      ],
      "metadata": {
        "id": "_wKP9oPoyNc8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UabtefhevXob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}